{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Data Science Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced Textbook: https://cobweb.cs.uga.edu/~jam/scalation_guide/comp_data_science.pdf\n",
    "\n",
    "Specifically Appendix A (Starting Page 619)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Reminder: Brush up on you Calculus especially derivatives and the chain rule ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is it?\n",
    "\n",
    "Gradient Descent is an algorithm that solves optimization problems using first-order iterations. Since it is designed to find the local minimum of a differential function, gradient descent is widely used in machine learning models to find the best parameters that minimize the modelâ€™s cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../pics/grad_dec/gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need it? \n",
    "\n",
    "Data science uses optimization to fit parameters in models, where for example a quality of fit measure (e.g.,sum of squared errors) is minimized. Typically, gradients are involved. In some cases, the gradient of the measure can be set to zero allowing the optimal parameters to be determined by matrix factorization. For complex models, this may not work, so an optimization algorithm that moves in the direction opposite to the gradient can be applied.\n",
    "\n",
    "In addition as the number of variables or features in our problem increases, it becomes more computationally expensive to use matrix factorization or other pure linear alegebra techniques. So we can use the gradient descent algoritm to save time in computation. Also, gradient descent allows for parrallization and distributed calculations across processors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "![](../pics/grad_dec/gd_work.png)\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Initialize the inputs (weights) randomly and select a learning rate\n",
    "1. Calculate the gradient \n",
    "1. Adjust the inputs (weights) with the gradients\n",
    "1. Use new inputs (weights) to repeats steps 2 and 3 until some condition (ie. inputs (weights) no longer significantly reduce error, max iterations, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "There are 2 flavors or ways to approach gradient decent.\n",
    "\n",
    "1. Pure Gradient Desent - You can update your inputs (weights) for each data instance\n",
    "2. Batch Gradient Desent - You can update your inputs (weights) for after going through the full training set with an average\n",
    "\n",
    "In our case, we will just be showing pure gradient descent and updating for every instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Example\n",
    "\n",
    "![](../pics/grad_dec/cost_fnt.png)\n",
    "\n",
    "Visual repersentation of the cost or objective function:\n",
    "\n",
    "![](../pics/grad_dec/cfg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. INITIALIZE INPUTS (WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by initializing our inputs (weights)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = [np.random.random() * 10, np.random.random() * 10]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. CALCULATE GRADIENT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Gradient in 2 Dimensions:\n",
    "\n",
    "![](../pics/grad_dec/grad.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating our gradient!\n",
    "# We can do this manually or with the python package sympy\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "# Define our initatial cost function or more generally our function to minimize\n",
    "# In this case we will let x1 = x and x2 = y\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "y = sp.Symbol('y')\n",
    "\n",
    "fox = (x - 4) ** 2 + (y - 2) ** 2\n",
    "\n",
    "dx_fox = sp.diff(fox, x)\n",
    "\n",
    "dy_fox = sp.diff(fox, y)\n",
    "\n",
    "grad_vec = [dx_fox, dy_fox]\n",
    "\n",
    "grad_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to return the value from the gradiant for a given input\n",
    "\n",
    "def getGrad (grad_vec, var, input):\n",
    "    \n",
    "    if (var == 0):\n",
    "        grad = grad_vec[var].evalf(subs={x:input})\n",
    "    else:\n",
    "        grad = grad_vec[var].evalf(subs={y:input})\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. UPDATE INPUTS (WEIGHTS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation showing how we should update our inputs (weights)\n",
    "\n",
    "![](../pics/grad_dec/gd_uf.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a gradient decent function from the above equation!\n",
    "# eta is our learning rate in this case and we will set it to .01\n",
    "\n",
    "def gradientDecent (X, eta):\n",
    "    X[0] = X[0] - eta * getGrad(grad_vec, 0, X[0])\n",
    "    X[1] = X[1] - eta * getGrad(grad_vec, 1, X[1])\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets put it in a loop to see how it behave over many iterations\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "X1_list = []\n",
    "X2_list = []\n",
    "\n",
    "for i in range (0, max_iter):\n",
    "    \n",
    "    X = gradientDecent (X , .1)\n",
    "    print(X)\n",
    "    X1_list.append(X[0])\n",
    "    X2_list.append(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our findings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make a list of 0 to 1000 so we can plot our iterations\n",
    "\n",
    "iters = range(0, max_iter)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iters, X1_list)\n",
    "plt.title('X1')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('X1 Inputs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(iters, X2_list)\n",
    "plt.title(\"X2\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('X2 Inputs')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
